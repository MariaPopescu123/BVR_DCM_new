---
title: "Magnitude Analysis"
output: html_document
---

This for final machine learning analysis. 

before running this script first run
01_DataDownload
03_Datawrangling: then go through all the files within chronologically
then you can run this script

1. prep data for RandomForest
2. tune model for RandomForest
3. run Random Forest
4. visualize SHAP values

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

load the packages that you need
```{r}
library(randomForest)
library(missForest)
library(caret)
library(future)
library(here)
```

This data frame has predictor values for DCM depth as well as DCM magnitude. 
Need to split these up for two different analysis

```{r}
full_weekly_data <- read.csv(here("CSVs", "full_weekly_data.csv"))

magnitude_analysis <- full_weekly_data|>
  select(-starts_with("depth_"), -Week, -totals_mean, -totals_med)
```

Tidy up the frame for depth analysis
```{r}
# Remove non-numeric columns (excluding Date, Depth_m, Year, etc.)
non_numeric_columns <- sapply(magnitude_analysis, function(x) !is.numeric(x) & !is.factor(x))
final_no_non_numeric <- magnitude_analysis |>
  select(-which(non_numeric_columns)) 

# Replace Inf and NaN with NA in all numeric columns
final_no_non_numeric <- final_no_non_numeric %>%
  mutate(across(where(is.numeric), ~ ifelse(is.infinite(.) | is.nan(.), NA, .)))

# Remove columns with more than 75% NA values
final_data_no_na <- final_no_non_numeric %>%
  select(where(~ mean(is.na(.)) <= 0.25))  # Keep columns with ≤ 25% NA

# Remove remaining rows with any NA values
RF_magnitude_analysis <- final_data_no_na %>%
  na.omit()

write.csv(RF_magnitude_analysis, here("CSVs", "RF_magnitude_analysis.csv"), row.names = FALSE)
```

can remove this after, want to see if I can make a PCA
IDK about this

```{r}
# Standardize your data
df <- scale(RF_magnitude_analysis)

# Run PCA
pca_result <- prcomp(df, center = TRUE, scale. = TRUE)

# Summary of PCA
summary(pca_result)

# Increase plot size and decrease font size
par(cex = 0.7, mar = c(5, 4, 2, 2))  # cex = font size, mar = margins

# Scree plot
plot(pca_result, main = "Scree Plot", cex.main = 0.9)

# Save as PNG
png(here("Figs", "pca_biplot.png"), width = 1200, height = 1000, res = 150)

# Set smaller font size
par(cex = 0.6)

# Plot
biplot(pca_result, main = "PCA Biplot")

# Close the graphics device
dev.off()

library(factoextra)

# Nice plot with auto-labeled axes
fviz_pca_biplot(pca_result,
                label = "var",     # show variable names only
                addEllipses = TRUE,
                repel = TRUE)
```



Grid search. We do not have enough data to split into training and test set. 
```{r}
set.seed(123)

# Initialize empty list to store results
results <- list()

# Counter for indexing
i <- 1

for (tree_num in c(100, 200, 300, 500)) {
  for (node_size in c(2, 4, 6, 8)) {
    for (mt in c(3, 6, 9, 10, 20)) {
      
      model_rf <- randomForest(
        max_conc ~ ., 
        data = RF_magnitude_analysis,
        ntree = tree_num,
        mtry = mt,
        nodesize = node_size,
        importance = TRUE
      )
      
      rsq_test <- mean(model_rf$rsq)
      mse_test <- mean(model_rf$mse)
      
      # Store results
      results[[i]] <- data.frame(
        Trees = tree_num,
        `Node size` = node_size,
        mtry = mt,
        `R-squared` = rsq_test,
        MSE = mse_test
      )
      
      i <- i + 1
    }
  }
}

# Combine into a single data frame
depth_RF_tuning_scores <- do.call(rbind, results)


depth_RF_tuning_scores <- depth_RF_tuning_scores |>
  arrange(desc(`R.squared`))

print(depth_RF_tuning_scores)

#this gives the best score
```  

Now to run the actual model
```{r}

test_model_rf <- randomForest(max_conc ~ .,
                                data = RF_magnitude_analysis,
                                ntree = 500,
                                node_size = 6,
                                mtry = 20,
                                importance = TRUE)
  
importance(test_model_rf)
```

Prep to visualize
```{r}
importance_df <- as.data.frame(importance(test_model_rf))
importance_df <- rownames_to_column(importance_df, var = "Variable") # Convert row names to a column
  
filtered_importance_df <- importance_df %>%
  filter(!is.na(`%IncMSE`), `%IncMSE` > 0)# Filter for valid and positive %IncMSE

rsq_test<- mean((model_rf$rsq))
mse_test<- mean((model_rf$mse))
```  
Need to at some point run VIF will come back to this

visualize
```{r}
# Create the plot
ggplot(filtered_importance_df, aes(x = `%IncMSE`, y = reorder(Variable, `%IncMSE`))) +
    geom_point(color = "blue", size = 3) +
    labs(
      title = "Variable Importance for on % IncMSE 2014-2023",
      x = "% IncMSE",
      y = "Variables"
    ) +
    theme_minimal()
```

#SHAP
SHAP: SHAP values (SHapley Additive exPlanations) are a way to explain machine learning model predictions by showing how much each feature contributes to a particular prediction.
```{r}
  library(fastshap)
  X = data.matrix(select(RF_magnitude_analysis, -max_conc))

  shap_values = fastshap::explain(test_model_rf, X=X, nsim=100, pred_wrapper=function(x,newdata){predict(x,newdata)})
  dim(shap_values)
  head(shap_values)
  
  preds = predict(test_model_rf, X)
  base_value = mean(preds)
  base_value
  cat(
    "shap+base_value:\t",sum(shap_values[1,]) + base_value,
    "\n     prediction:\t", preds[1]
  )

  as_tibble(X) %>% rownames_to_column('row_id') %>%
    pivot_longer(names_to='var', values_to='value', -row_id) -> vars
  as_tibble(shap_values) %>% rownames_to_column('row_id') %>%
    pivot_longer(names_to='var', values_to='shap', -row_id) -> shaps
  df = inner_join(vars, shaps, by=c('row_id', 'var'))
  head(df)
  
  # Check structure of relevant columns
  str(df)
  
  # Optional: Convert to atomic vectors just in case
  df <- df %>%
    mutate(
      shap = as.numeric(shap),
      value = as.character(value),  # or as.numeric if needed
      var = as.character(var)
    )
  
  # Now run plot
  df %>%
    filter(row_id == 1) %>%
    ggplot(aes(x = shap, y = fct_reorder(paste0(var, "=", value), shap), fill = factor(sign(shap)))) +
    geom_col() +
    guides(fill = 'none') +
    labs(y = "", title = "SHAP values for X[1,]")
  

library(ggbeeswarm)
  df %>%
  mutate(value = as.numeric(value)) %>%
  group_by(var) %>%
  mutate(nv = scale(value)) %>%
  ggplot(aes(x = shap, y = var, color = nv)) +
  geom_quasirandom(groupOnX = FALSE, dodge.width = 0.3) +
  scale_color_viridis_c(option = 'H', limits = c(-3, 3), oob = scales::oob_squish) +
  labs(
    title = 'Distribution of SHAP values for all samples 2014-2024',
    y = '',
    color = 'z-scaled values'
  )
    group_by(df, var) %>% 
    summarize(mean=mean(abs(shap))) %>%
    ggplot(aes(x=mean, y=fct_reorder(var, mean))) + 
    geom_col() +
    labs(x='mean(|shap value|)', title='mean absolute shap for all samples', y="")
  
  group_by(df, var, sign=factor(sign(shap))) %>%
    summarize(mean=mean(shap)) %>%
    ggplot(aes(x=mean, y=fct_reorder(var, mean), fill=sign)) + 
    geom_col() +
    labs(x='mean(shap value)', title='mean shap for all samples 2014-2023', y="")
```

now to look at specific variables and their values
```{r}
library(ggplot2)
library(dplyr)
library(scales)
library(here)

# Define the list of variables
vars_to_plot <- c('WaterLevel_m', 'avg_airtemp_lagged', 'SMn_mgL_max_val', 'PZ', 'DCM_depth', 'np_ratio_range')

# Loop and save each plot
for (v in vars_to_plot) {
  
  # Create plot
  p <- df %>%
    filter(var == v) %>%
    mutate(value = as.numeric(value)) %>%
    ggplot(aes(x = value, y = shap)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    scale_x_continuous(breaks = pretty_breaks(n = 6)) +
    labs(
      title = paste0('2014–2023 Interaction: SHAP vs ', v),
      x = v,
      y = "SHAP value"
    )
  
  # Print inline for RMarkdown rendering
  print(p)

  # Save to file
  ggsave(
    filename = here::here("Figs/xai_plots", paste0("magnitude_2014_2023shap_", v, ".png")),
    plot = p,
    width = 6,
    height = 4,
    dpi = 300
  )
}
```

