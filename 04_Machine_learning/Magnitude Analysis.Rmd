---
title: "Magnitude Analysis"
output: html_document
---

This for final machine learning analysis. 

before running this script first run
01_DataDownload
03_Datawrangling: then go through all the files within chronologically
then you can run this script

1. prep data for RandomForest
2. tune model for RandomForest
3. run Random Forest
4. visualize SHAP values

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

load the packages that you need
```{r}
library(randomForest)
library(missForest)
library(caret)
library(future)
library(here)
```

This data frame has predictor values for DCM depth as well as DCM magnitude. 
Need to split these up for two different analysis

```{r}
full_weekly_data <- read.csv(here("CSVs", "full_weekly_data.csv"))

magnitude_analysis <- full_weekly_data|>
  select(max_conc, WaterLevel_m, PZ, TFe_mgL_max_val,TFe_mgL_min_val, avg_airtemp_lagged, np_ratio_range, np_ratio_max_val, schmidt_stability, thermocline_depth)
```

Tidy up the frame for depth analysis
```{r}
# Remove non-numeric columns (excluding Date, Depth_m, Year, etc.)
non_numeric_columns <- sapply(magnitude_analysis, function(x) !is.numeric(x) & !is.factor(x))
final_no_non_numeric <- magnitude_analysis |>
  select(-which(non_numeric_columns)) 

# Replace Inf and NaN with NA in all numeric columns
final_no_non_numeric <- final_no_non_numeric %>%
  mutate(across(where(is.numeric), ~ ifelse(is.infinite(.) | is.nan(.), NA, .)))

# Remove columns with more than 75% NA values
final_data_no_na <- final_no_non_numeric %>%
  select(where(~ mean(is.na(.)) <= 0.25))  # Keep columns with ≤ 25% NA

# Remove remaining rows with any NA values
RF_magnitude_analysis <- final_data_no_na %>%
  na.omit()

write.csv(RF_magnitude_analysis, here("CSVs", "RF_magnitude_analysis.csv"), row.names = FALSE)

#see how much is actualyl going in
test<-magnitude_analysis |>
    select(where(~ mean(is.na(.)) <= 0.25))  # Keep columns with ≤ 25% NA

RF_frame_w_dates <- test %>%
  na.omit()
 
# RF_frame_w_dates %>%
#   count(year(Date)) %>%
#   print() 
#   
```

can remove this after, want to see if I can make a PCA
IDK about this

```{r}
# Standardize your data
df <- scale(RF_magnitude_analysis)

# Run PCA
pca_result <- prcomp(df, center = TRUE, scale. = TRUE)

# Summary of PCA
summary(pca_result)

# Increase plot size and decrease font size
par(cex = 0.7, mar = c(5, 4, 2, 2))  # cex = font size, mar = margins

# Scree plot
plot(pca_result, main = "Scree Plot", cex.main = 0.9)

# Save as PNG
png(here("Figs", "pca_biplot.png"), width = 1200, height = 1000, res = 150)

# Set smaller font size
par(cex = 0.6)

# Plot
biplot(pca_result, main = "PCA Biplot")

# Close the graphics device
dev.off()

library(factoextra)

# Nice plot with auto-labeled axes
fviz_pca_biplot(pca_result,
                label = "var",     # show variable names only
                addEllipses = TRUE,
                repel = TRUE)
```



Grid search. We do not have enough data to split into training and test set. 
```{r}
set.seed(123)

# Initialize empty list to store results
results <- list()

# Counter for indexing
i <- 1

for (tree_num in c(100, 200, 300, 500)) {
  for (node_size in c(2, 4, 6, 8)) {
    for (mt in c(3, 6, 9, 10, 20)) {
      
      model_rf <- randomForest(
        max_conc ~ ., 
        data = RF_magnitude_analysis,
        ntree = tree_num,
        mtry = mt,
        nodesize = node_size,
        importance = TRUE
      )
      
preds <- predict(model_rf, RF_magnitude_analysis)
obs <- RF_magnitude_analysis$max_conc
rsq_test <- 1 - sum((obs - preds)^2) / sum((obs - mean(obs))^2)
mse_test <- mean(model_rf$mse)
    
      # Store results
      results[[i]] <- data.frame(
        Trees = tree_num,
        `Node size` = node_size,
        mtry = mt,
        `R-squared` = rsq_test,
        MSE = mse_test
      )
      
      i <- i + 1
    }
  }
}

# Combine into a single data frame
depth_RF_tuning_scores <- do.call(rbind, results)


depth_RF_tuning_scores <- depth_RF_tuning_scores |>
  arrange(desc(`R.squared`))

print(depth_RF_tuning_scores)

#this gives the best score
```  

Now to run the actual model
```{r}

test_model_rf <- randomForest(max_conc ~ .,
                                data = RF_magnitude_analysis,
                                ntree = 200,
                                node_size = 2,
                                mtry = 3,
                                importance = TRUE)
  
importance(test_model_rf)
```

Prep to visualize
```{r}
importance_df <- as.data.frame(importance(test_model_rf))
importance_df <- rownames_to_column(importance_df, var = "Variable") # Convert row names to a column
  
filtered_importance_df <- importance_df %>%
  filter(!is.na(`%IncMSE`), `%IncMSE` > 0)# Filter for valid and positive %IncMSE

rsq_test<- mean((model_rf$rsq))
mse_test<- mean((model_rf$mse))
```  
Need to at some point run VIF will come back to this

visualize
```{r save-plot, eval=TRUE}
#need to fix this

var_importance_depth_plot <- ggplot(filtered_importance_df, aes(x = `%IncMSE`, y = reorder(Variable, `%IncMSE`))) +
  geom_point(color = "blue", size = 3) +
  labs(
    title = "Variable Importance for Magnitude Prediction 2014–2023",
    x = "% IncMSE",
    y = "Variables"
  ) +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), limits = c(0, NA)) +
  theme(
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    plot.title = element_text(size = 25, hjust = .5),
    plot.title.position = "plot",
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 16),
    plot.margin = unit(c(10, 20, 10, 5), "pt")
  )
# Save to a relative directory
ggsave("var_importance_magnitude_plot.png", plot = var_importance_depth_plot, width = 10, height = 4, dpi = 600)

print(var_importance_depth_plot)
```

#SHAP
SHAP: SHAP values (SHapley Additive exPlanations) are a way to explain machine learning model predictions by showing how much each feature contributes to a particular prediction.

```{r}
library(fastshap)
library(tidyverse)
library(ggbeeswarm)
library(viridis)
library(forcats)

# Prepare X matrix for shap
X <- data.matrix(select(RF_magnitude_analysis, -max_conc))

# Calculate SHAP values
shap_values <- fastshap::explain(
  test_model_rf, X = X, nsim = 100,
  pred_wrapper = function(x, newdata) predict(x, newdata)
)

# Predicted values and base value
preds <- predict(test_model_rf, X)
base_value <- mean(preds)

cat("shap + base_value:\t", sum(shap_values[1,]) + base_value,
    "\n     prediction:\t", preds[1], "\n")

# Prepare tidy df with feature values and shap values
vars <- as_tibble(X) %>%
  rownames_to_column('row_id') %>%
  pivot_longer(-row_id, names_to = 'var', values_to = 'value')

shaps <- as_tibble(shap_values) %>%
  rownames_to_column('row_id') %>%
  pivot_longer(-row_id, names_to = 'var', values_to = 'shap')

df <- inner_join(vars, shaps, by = c('row_id', 'var'))

# Clean up types
df <- df %>%
  mutate(
    shap = as.numeric(shap),
    value = as.character(value),  # or as.numeric(value) if appropriate
    var = as.character(var)
  )

# Variable ordering for plots
ordered_vars <- filtered_importance_df %>%
  arrange(desc(`%IncMSE`)) %>%
  pull(Variable) %>%
  rev()

# Plot 0: SHAP values for row 1 (the one you liked the most)
plot_shap_1 <- df %>%
  filter(row_id == 1) %>%
  ggplot(aes(x = shap, y = fct_reorder(paste0(var, "=", value), shap), fill = factor(sign(shap)))) +
  geom_col() +
  guides(fill = 'none') +
  labs(y = "", title = "SHAP values for X[1,]") +
  theme_minimal(base_size = 16)

# Plot 1: Distribution of SHAP values with z-scaled coloring
plot_dist <- df %>%
  filter(var %in% ordered_vars) %>%
  mutate(
    value = as.numeric(value),
    var = factor(var, levels = ordered_vars)) %>%
  group_by(var) %>%
mutate(nv = scale(value))%>%
  ungroup()%>%
  ggplot(aes(x = shap, y = var, color = nv)) +
  geom_quasirandom(groupOnX = FALSE, dodge.width = 0.3) +
  scale_color_viridis_c(option = 'H', limits = c(-3, 3), oob = scales::oob_squish) +
  labs(
    title = 'Distribution of SHAP values for Magnitude Prediction',
    y = '',
    color = 'z-scaled values'
  ) +
theme(
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    plot.title = element_text(size = 25, hjust = .5),  # Left-align title
    plot.title.position = "plot",  # Positions the title relative to the entire plot area
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 16),
    legend.text = element_text(size = 14),     # <-- Increase legend text size
    legend.title = element_text(size = 16),
    plot.margin = margin(t = 10, r = 20, b = 10, l = 5)  # Reduce left margin if needed
  )
# Plot 2: Mean absolute SHAP value
# plot_mean_abs <- df %>%
#   group_by(var) %>%
#   summarize(mean = mean(abs(shap), na.rm = TRUE)) %>%
#   ggplot(aes(x = mean, y = fct_reorder(var, mean))) +
#   geom_col(fill = "steelblue") +
#   labs(
#     x = 'mean(|SHAP value|)',
#     title = 'Mean absolute SHAP for all samples',
#     y = ''
#   ) +
#   theme_minimal(base_size = 16)

# # Plot 3: Mean SHAP by sign
# plot_mean_sign <- df %>%
#   group_by(var, sign = factor(sign(shap))) %>%
#   summarize(mean = mean(shap, na.rm = TRUE)) %>%
#   ggplot(aes(x = mean, y = fct_reorder(var, mean), fill = sign)) +
#   geom_col() +
#   scale_fill_manual(values = c("-1" = "red", "0" = "gray", "1" = "green")) +
#   labs(
#     x = 'mean(SHAP value)',
#     title = 'Mean SHAP for all samples 2014–2023',
#     y = '',
#     fill = 'Sign'
#   ) +
#   theme_minimal(base_size = 16)

# Now you can save plots separately like this:
#dir.create("Figs", showWarnings = FALSE)

#ggsave("shap_values_row1.png", plot_shap_1, width = 8, height = 4, dpi = 600, bg = "white")
ggsave("magnitude_shap_distribution.png", plot_dist, width = 10, height = 4, dpi = 600, bg = "white")
#ggsave("mean_abs_shap.png", plot_mean_abs, width = 10, height = 6, dpi = 600, bg = "white")
#ggsave("mean_shap_by_sign.png", plot_mean_sign, width = 10, height = 6, dpi = 600, bg = "white")

#print(plot_shap_1)
print(plot_dist)
#print(plot_mean_abs)
#print(plot_mean_sign))

```


OLD CHUNK
```{r}
  library(fastshap)
  X = data.matrix(select(RF_magnitude_analysis, -max_conc))

  shap_values = fastshap::explain(test_model_rf, X=X, nsim=100, pred_wrapper=function(x,newdata){predict(x,newdata)})
  dim(shap_values)
  head(shap_values)
  
  preds = predict(test_model_rf, X)
  base_value = mean(preds)
  base_value
  cat(
    "shap+base_value:\t",sum(shap_values[1,]) + base_value,
    "\n     prediction:\t", preds[1]
  )

  as_tibble(X) %>% rownames_to_column('row_id') %>%
    pivot_longer(names_to='var', values_to='value', -row_id) -> vars
  as_tibble(shap_values) %>% rownames_to_column('row_id') %>%
    pivot_longer(names_to='var', values_to='shap', -row_id) -> shaps
  df = inner_join(vars, shaps, by=c('row_id', 'var'))
  head(df)
  
  # Check structure of relevant columns
  str(df)
  
  # Optional: Convert to atomic vectors just in case
  df <- df %>%
    mutate(
      shap = as.numeric(shap),
      value = as.character(value),  # or as.numeric if needed
      var = as.character(var)
    )
  
  # Now run plot
  df %>%
    filter(row_id == 1) %>%
    ggplot(aes(x = shap, y = fct_reorder(paste0(var, "=", value), shap), fill = factor(sign(shap)))) +
    geom_col() +
    guides(fill = 'none') +
    labs(y = "", title = "SHAP values for X[1,]")
  

ordered_vars <- filtered_importance_df %>%
  arrange(desc(`%IncMSE`)) %>%
  pull(Variable) %>%
  rev()

library(ggbeeswarm)

df %>%
  filter(var %in% ordered_vars) %>%  # remove rows not in ordered_vars
  mutate(
    value = as.numeric(value),
    var = factor(var, levels = ordered_vars)  # enforce reversed order
  ) %>%
  group_by(var) %>%
  mutate(nv = scale(value)) %>%
  ggplot(aes(x = shap, y = var, color = nv)) +
  geom_quasirandom(groupOnX = FALSE, dodge.width = 0.3) +
  scale_color_viridis_c(option = 'H', limits = c(-3, 3), oob = scales::oob_squish) +
  labs(
    title = 'Distribution of SHAP values for all samples 2014–2024',
    y = '',
    color = 'z-scaled values'
  )
    group_by(df, var) %>% 
    summarize(mean=mean(abs(shap))) %>%
    ggplot(aes(x=mean, y=fct_reorder(var, mean))) + 
    geom_col() +
    labs(x='mean(|shap value|)', title='mean absolute shap for all samples', y="")
  
  group_by(df, var, sign=factor(sign(shap))) %>%
    summarize(mean=mean(shap)) %>%
    ggplot(aes(x=mean, y=fct_reorder(var, mean), fill=sign)) + 
    geom_col() +
    labs(x='mean(shap value)', title='mean shap for all samples 2014-2023', y="")
```

now to look at specific variables and their values
```{r}
library(ggplot2)
library(dplyr)
library(scales)
library(here)

# Define the list of variables
vars_to_plot <- c('WaterLevel_m', 'avg_airtemp_lagged', 'SMn_mgL_max_val', 'PZ', 'DCM_depth', 'np_ratio_range')

# Loop and save each plot
for (v in vars_to_plot) {
  
  # Create plot
  p <- df %>%
    filter(var == v) %>%
    mutate(value = as.numeric(value)) %>%
    ggplot(aes(x = value, y = shap)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    scale_x_continuous(breaks = pretty_breaks(n = 6)) +
    labs(
      title = paste0('2014–2023 Interaction: SHAP vs ', v),
      x = v,
      y = "SHAP value"
    )
  
  # Print inline for RMarkdown rendering
  print(p)

  # Save to file
  ggsave(
    filename = here::here("Figs/xai_plots", paste0("magnitude_2014_2023shap_", v, ".png")),
    plot = p,
    width = 6,
    height = 4,
    dpi = 300
  )
}
```

