---
title: "Maching_Learning_viz"
author: "Maria Popescu"
date: "2025-05-30"
output: html_document
---
This for final machine learning analysis. 

before running this script first run
01_DataDownload
03_Datawrangling: then go through all the files within chronologically
then you can run this script

1. prep data for RandomForest
2. tune model for RandomForest
3. run Random Forest
4. visualize SHAP values

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

load the packages that you need
```{r}
library(randomForest)
library(missForest)
library(caret)
library(future)
```

This data frame has predictor values for DCM depth as well as DCM magnitude. 
Need to split these up for two different analysis
```{r}
full_weekly_data <- read.csv("CSVs/full_weekly_data.csv")

#print(colnames(full_weekly_data_clean))

depth_analysis <- full_weekly_data |>
  select(-ends_with("max_val"), -ends_with("min_val"), -ends_with("range"), 
         -max_conc, -totals_mean, -totals_med, -N_at_DCM, -Week)
  
magnitude_analysis <- full_weekly_data|>
  select(-starts_with("depth_"), -PZ, -Week)
```

Tidy up the frame for depth analysis
```{r}
# Remove non-numeric columns (excluding Date, Depth_m, Year, etc.)
non_numeric_columns <- sapply(depth_analysis, function(x) !is.numeric(x) & !is.factor(x))
final_no_non_numeric <- depth_analysis |>
  select(-which(non_numeric_columns)) 

# Replace Inf and NaN with NA in all numeric columns
final_no_non_numeric <- final_no_non_numeric %>%
  mutate(across(where(is.numeric), ~ ifelse(is.infinite(.) | is.nan(.), NA, .)))

# Remove columns with more than 75% NA values
final_data_no_na <- final_no_non_numeric %>%
  select(where(~ mean(is.na(.)) <= 0.25))  # Keep columns with â‰¤ 25% NA

# Remove remaining rows with any NA values
RF_depth_analysis <- final_data_no_na %>%
  na.omit()

write.csv(RF_depth_analysis, "CSVs/RF_depth_analysis.csv", row.names = FALSE)
```

Grid search. We do not have enough data to split into training and test set. 
```{r}
set.seed(123)

# Initialize empty list to store results
results <- list()

# Counter for indexing
i <- 1

for (tree_num in c(100, 200, 300, 500)) {
  for (node_size in c(2, 4, 6, 8)) {
    for (mt in c(3, 6, 9, 10, 20)) {
      
      model_rf <- randomForest(
        DCM_depth ~ ., 
        data = RF_depth_analysis,
        ntree = tree_num,
        mtry = mt,
        nodesize = node_size,
        importance = TRUE
      )
      
      rsq_test <- mean(model_rf$rsq)
      mse_test <- mean(model_rf$mse)
      
      # Store results
      results[[i]] <- data.frame(
        Trees = tree_num,
        `Node size` = node_size,
        mtry = mt,
        `R-squared` = rsq_test,
        MSE = mse_test
      )
      
      i <- i + 1
    }
  }
}

# Combine into a single data frame
depth_RF_tuning_scores <- do.call(rbind, results)


depth_RF_tuning_scores <- depth_RF_tuning_scores |>
  arrange(desc(`R.squared`))

#this gives the best score
```  

Now to run the actual model
```{r}

test_model_rf <- randomForest(DCM_depth ~ .,
                                data = RF_depth_analysis,
                                ntree = 500,
                                node_size = 6,
                                mtry = 20,
                                importance = TRUE)
  
importance(test_model_rf)
```

Prep to visualize
```{r}
importance_df <- as.data.frame(importance(test_model_rf))
importance_df <- rownames_to_column(importance_df, var = "Variable") # Convert row names to a column
  
filtered_importance_df <- importance_df %>%
  filter(!is.na(`%IncMSE`), `%IncMSE` > 0)# Filter for valid and positive %IncMSE

rsq_test<- mean((model_rf$rsq))
mse_test<- mean((model_rf$mse))
```  
Need to at some point run VIF will come back to this

visualize
```{r}
# Create the plot
ggplot(filtered_importance_df, aes(x = `%IncMSE`, y = reorder(Variable, `%IncMSE`))) +
    geom_point(color = "blue", size = 3) +
    labs(
      title = "Variable Importance based on % IncMSE 2014-2023",
      x = "% IncMSE",
      y = "Variables"
    ) +
    theme_minimal()
```

#SHAP
SHAP: SHAP values (SHapley Additive exPlanations) are a way to explain machine learning model predictions by showing how much each feature contributes to a particular prediction.
```{r}
  library(fastshap)
  X = data.matrix(select(RF_depth_analysis, -DCM_depth))

  shap_values = fastshap::explain(test_model_rf, X=X, nsim=100, pred_wrapper=function(x,newdata){predict(x,newdata)})
  dim(shap_values)
  head(shap_values)
  
  preds = predict(test_model_rf, X)
  base_value = mean(preds)
  base_value
  cat(
    "shap+base_value:\t",sum(shap_values[1,]) + base_value,
    "\n     prediction:\t", preds[1]
  )

  as_tibble(X) %>% rownames_to_column('row_id') %>%
    pivot_longer(names_to='var', values_to='value', -row_id) -> vars
  as_tibble(shap_values) %>% rownames_to_column('row_id') %>%
    pivot_longer(names_to='var', values_to='shap', -row_id) -> shaps
  df = inner_join(vars, shaps, by=c('row_id', 'var'))
  head(df)
  
  # Check structure of relevant columns
  str(df)
  
  # Optional: Convert to atomic vectors just in case
  df <- df %>%
    mutate(
      shap = as.numeric(shap),
      value = as.character(value),  # or as.numeric if needed
      var = as.character(var)
    )
  
  # Now run plot
  df %>%
    filter(row_id == 1) %>%
    ggplot(aes(x = shap, y = fct_reorder(paste0(var, "=", value), shap), fill = factor(sign(shap)))) +
    geom_col() +
    guides(fill = 'none') +
    labs(y = "", title = "SHAP values for X[1,]")
  

library(ggbeeswarm)
  df %>%
  mutate(value = as.numeric(value)) %>%
  group_by(var) %>%
  mutate(nv = scale(value)) %>%
  ggplot(aes(x = shap, y = var, color = nv)) +
  geom_quasirandom(groupOnX = FALSE, dodge.width = 0.3) +
  scale_color_viridis_c(option = 'H', limits = c(-3, 3), oob = scales::oob_squish) +
  labs(
    title = 'Distribution of SHAP values for all samples',
    y = '',
    color = 'z-scaled values'
  )
    group_by(df, var) %>% 
    summarize(mean=mean(abs(shap))) %>%
    ggplot(aes(x=mean, y=fct_reorder(var, mean))) + 
    geom_col() +
    labs(x='mean(|shap value|)', title='mean absolute shap for all samples', y="")
  
  group_by(df, var, sign=factor(sign(shap))) %>%
    summarize(mean=mean(shap)) %>%
    ggplot(aes(x=mean, y=fct_reorder(var, mean), fill=sign)) + 
    geom_col() +
    labs(x='mean(shap value)', title='mean shap for all samples', y="")
```

now to look at specific variables and their values
```{r}
  #this not showing 
  filter(df, var=='PZ') %>%
    ggplot(aes(x=value, y=shap)) + geom_point() +
    geom_smooth() +
    labs(title='interaction shap vs PZ', x='DCM_Depth')
  
  ggplot(aes(x=value, y=shap)) + geom_point() + 
    geom_smooth() +
    labs(title='interaction shap vs rm', x='rm')
```
